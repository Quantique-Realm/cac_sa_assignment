{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eebaffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T06:59:59.278483Z",
     "iopub.status.busy": "2025-06-16T06:59:59.278219Z",
     "iopub.status.idle": "2025-06-16T07:01:24.494123Z",
     "shell.execute_reply": "2025-06-16T07:01:24.492884Z"
    },
    "papermill": {
     "duration": 85.221508,
     "end_time": "2025-06-16T07:01:24.495674",
     "exception": false,
     "start_time": "2025-06-16T06:59:59.274166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 NDVI columns\n",
      "Applying smoothing...\n",
      "Adding comprehensive features...\n",
      "Adding comprehensive features...\n",
      "Total features created: 35\n",
      "Target classes: ['water', 'forest', 'impervious', 'farm', 'grass', 'orchard']\n",
      "Selected 25 best features\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON\n",
      "============================================================\n",
      "\n",
      "1. Testing Logistic Regression convergence:\n",
      "----------------------------------------\n",
      "max_iter= 500: CV Accuracy = 0.8691 ± 0.0049\n",
      "max_iter=1000: CV Accuracy = 0.8691 ± 0.0049\n",
      "max_iter=2000: CV Accuracy = 0.8691 ± 0.0049\n",
      "Best Logistic Regression: max_iter=500, score=0.8691\n",
      "\n",
      "2. Testing Random Forest:\n",
      "----------------------------------------\n",
      "Random Forest: CV Accuracy = 0.8800 ± 0.0064\n",
      "\n",
      "3. Hyperparameter Tuning:\n",
      "----------------------------------------\n",
      "Random Forest performed better - tuning RF parameters...\n",
      "Best RF params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best RF score: 0.8580\n",
      "\n",
      "============================================================\n",
      "FINAL PREDICTION\n",
      "============================================================\n",
      "Training final model with CV score: 0.8580\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "      feature  importance\n",
      "    log_range    0.100295\n",
      "   ndvi_range    0.093311\n",
      "     ndvi_std    0.079751\n",
      "      log_std    0.062236\n",
      "     ndvi_max    0.058207\n",
      "extreme_range    0.055686\n",
      "     ndvi_p90    0.053032\n",
      "   std_x_mean    0.042136\n",
      "     ndvi_min    0.032766\n",
      "     ndvi_p10    0.030651\n",
      "\n",
      "Saved submission.csv with 2845 predictions\n",
      "Model training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scipy.signal import savgol_filter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktrain.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv\")\n",
    "submission = pd.DataFrame()\n",
    "submission['ID'] = test_df['ID']\n",
    "\n",
    "# NDVI columns\n",
    "ndvi_cols = [col for col in train_df.columns if col.endswith('_N')]\n",
    "print(f\"Found {len(ndvi_cols)} NDVI columns\")\n",
    "\n",
    "# Convert safely\n",
    "train_df[ndvi_cols] = train_df[ndvi_cols].apply(pd.to_numeric, errors='coerce')\n",
    "test_df[ndvi_cols] = test_df[ndvi_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fill NA - more efficient\n",
    "for col in ndvi_cols:\n",
    "    train_df[col] = train_df[col].fillna(train_df[col].median())\n",
    "    test_df[col] = test_df[col].fillna(test_df[col].median())\n",
    "\n",
    "# Optional: Smoothing (comment out if too slow)\n",
    "def smooth_ndvi_fast(df):\n",
    "    smoothed_df = df.copy()\n",
    "    for col in ndvi_cols:\n",
    "        if len(df) > 5:\n",
    "            try:\n",
    "                smoothed_df[col] = savgol_filter(df[col], min(5, len(df)//2*2+1), 2)\n",
    "            except:\n",
    "                pass\n",
    "    return smoothed_df\n",
    "\n",
    "print(\"Applying smoothing...\")\n",
    "train_df = smooth_ndvi_fast(train_df)\n",
    "test_df = smooth_ndvi_fast(test_df)\n",
    "\n",
    "# Enhanced Feature Engineering\n",
    "def add_comprehensive_features(df):\n",
    "    print(\"Adding comprehensive features...\")\n",
    "    \n",
    "    # Basic statistical features\n",
    "    df['ndvi_mean'] = df[ndvi_cols].mean(axis=1)\n",
    "    df['ndvi_std'] = df[ndvi_cols].std(axis=1)\n",
    "    df['ndvi_min'] = df[ndvi_cols].min(axis=1)\n",
    "    df['ndvi_max'] = df[ndvi_cols].max(axis=1)\n",
    "    df['ndvi_range'] = df['ndvi_max'] - df['ndvi_min']\n",
    "    df['ndvi_median'] = df[ndvi_cols].median(axis=1)\n",
    "    df['ndvi_skew'] = df[ndvi_cols].skew(axis=1)\n",
    "    df['ndvi_kurtosis'] = df[ndvi_cols].kurtosis(axis=1)\n",
    "    df['ndvi_q25'] = df[ndvi_cols].quantile(0.25, axis=1)\n",
    "    df['ndvi_q75'] = df[ndvi_cols].quantile(0.75, axis=1)\n",
    "    df['ndvi_iqr'] = df['ndvi_q75'] - df['ndvi_q25']\n",
    "    \n",
    "    # Trend analysis\n",
    "    df['ndvi_trend'] = df[ndvi_cols].apply(lambda row: np.polyfit(range(len(row)), row, 1)[0], axis=1)\n",
    "    \n",
    "    # Seasonal patterns\n",
    "    thirds = np.array_split(ndvi_cols, 3)\n",
    "    df['early_mean'] = df[thirds[0]].mean(axis=1)\n",
    "    df['mid_mean'] = df[thirds[1]].mean(axis=1)\n",
    "    df['late_mean'] = df[thirds[2]].mean(axis=1)\n",
    "    df['early_trend'] = df[thirds[0]].apply(lambda row: np.polyfit(range(len(row)), row, 1)[0], axis=1)\n",
    "    df['mid_trend'] = df[thirds[1]].apply(lambda row: np.polyfit(range(len(row)), row, 1)[0], axis=1)\n",
    "    df['late_trend'] = df[thirds[2]].apply(lambda row: np.polyfit(range(len(row)), row, 1)[0], axis=1)\n",
    "    \n",
    "    # Advanced seasonal features\n",
    "    df['early_late_diff'] = df['early_mean'] - df['late_mean']\n",
    "    df['peak_position'] = df[ndvi_cols].apply(lambda row: np.argmax(row) / len(row), axis=1)\n",
    "    df['valley_position'] = df[ndvi_cols].apply(lambda row: np.argmin(row) / len(row), axis=1)\n",
    "    df['peak_valley_diff'] = df['peak_position'] - df['valley_position']\n",
    "    \n",
    "    # Growth rate and acceleration\n",
    "    def calculate_acceleration(row):\n",
    "        if len(row) < 3:\n",
    "            return 0\n",
    "        diff1 = np.diff(row)\n",
    "        diff2 = np.diff(diff1)\n",
    "        return np.mean(diff2) if len(diff2) > 0 else 0\n",
    "    \n",
    "    df['ndvi_acceleration'] = df[ndvi_cols].apply(calculate_acceleration, axis=1)\n",
    "    \n",
    "    # Stability measures\n",
    "    df['ndvi_cv'] = df['ndvi_std'] / (df['ndvi_mean'] + 1e-8)  # Coefficient of variation\n",
    "    df['above_median_ratio'] = df[ndvi_cols].apply(lambda row: (row > row.median()).sum() / len(row), axis=1)\n",
    "    \n",
    "    # Interaction features\n",
    "    df['trend_x_mean'] = df['ndvi_trend'] * df['ndvi_mean']\n",
    "    df['range_x_skew'] = df['ndvi_range'] * df['ndvi_skew']\n",
    "    df['std_x_mean'] = df['ndvi_std'] * df['ndvi_mean']\n",
    "    df['peak_pos_x_max'] = df['peak_position'] * df['ndvi_max']\n",
    "    \n",
    "    # Log transformations for skewed features\n",
    "    df['log_std'] = np.log1p(df['ndvi_std'])\n",
    "    df['log_range'] = np.log1p(df['ndvi_range'])\n",
    "    df['log_iqr'] = np.log1p(df['ndvi_iqr'])\n",
    "    \n",
    "    # Percentile-based features\n",
    "    df['ndvi_p10'] = df[ndvi_cols].quantile(0.1, axis=1)\n",
    "    df['ndvi_p90'] = df[ndvi_cols].quantile(0.9, axis=1)\n",
    "    df['extreme_range'] = df['ndvi_p90'] - df['ndvi_p10']\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = add_comprehensive_features(train_df)\n",
    "test_df = add_comprehensive_features(test_df)\n",
    "\n",
    "# Comprehensive feature list\n",
    "features = [\n",
    "    # Basic statistics\n",
    "    'ndvi_mean', 'ndvi_std', 'ndvi_min', 'ndvi_max', 'ndvi_range',\n",
    "    'ndvi_median', 'ndvi_skew', 'ndvi_kurtosis', 'ndvi_q25', 'ndvi_q75', 'ndvi_iqr',\n",
    "    \n",
    "    # Trends\n",
    "    'ndvi_trend', 'early_trend', 'mid_trend', 'late_trend',\n",
    "    \n",
    "    # Seasonal\n",
    "    'early_mean', 'mid_mean', 'late_mean', 'early_late_diff',\n",
    "    'peak_position', 'valley_position', 'peak_valley_diff',\n",
    "    \n",
    "    # Advanced\n",
    "    'ndvi_acceleration', 'ndvi_cv', 'above_median_ratio',\n",
    "    \n",
    "    # Interactions\n",
    "    'trend_x_mean', 'range_x_skew', 'std_x_mean', 'peak_pos_x_max',\n",
    "    \n",
    "    # Transformations\n",
    "    'log_std', 'log_range', 'log_iqr',\n",
    "    \n",
    "    # Percentiles\n",
    "    'ndvi_p10', 'ndvi_p90', 'extreme_range'\n",
    "]\n",
    "\n",
    "print(f\"Total features created: {len(features)}\")\n",
    "\n",
    "X = train_df[features]\n",
    "y = train_df['class']\n",
    "X_test = test_df[features]\n",
    "\n",
    "# Handle any infinite or NaN values\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Encode target\n",
    "label_map = {label: i for i, label in enumerate(y.unique())}\n",
    "inverse_label_map = {i: label for label, i in label_map.items()}\n",
    "y_encoded = y.map(label_map)\n",
    "\n",
    "print(f\"Target classes: {list(label_map.keys())}\")\n",
    "\n",
    "# Feature Selection - Select best features\n",
    "selector = SelectKBest(score_func=f_classif, k=min(25, len(features)))\n",
    "X_selected = selector.fit_transform(X, y_encoded)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "selected_features = [features[i] for i in selector.get_support(indices=True)]\n",
    "print(f\"Selected {len(selected_features)} best features\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=100)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Test Logistic Regression with different max_iter\n",
    "print(\"\\n1. Testing Logistic Regression convergence:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "iter_values = [500, 1000, 2000]\n",
    "best_lr_score = 0\n",
    "best_lr_iter = 500\n",
    "\n",
    "for max_iter in iter_values:\n",
    "    logreg = LogisticRegression(\n",
    "        max_iter=max_iter, \n",
    "        solver='lbfgs', \n",
    "        multi_class='multinomial', \n",
    "        random_state=100\n",
    "    )\n",
    "    \n",
    "    scores = cross_val_score(logreg, X_scaled, y_encoded, cv=cv, scoring='accuracy')\n",
    "    score_mean = scores.mean()\n",
    "    \n",
    "    print(f\"max_iter={max_iter:4d}: CV Accuracy = {score_mean:.4f} ± {scores.std():.4f}\")\n",
    "    \n",
    "    if score_mean > best_lr_score:\n",
    "        best_lr_score = score_mean\n",
    "        best_lr_iter = max_iter\n",
    "\n",
    "print(f\"Best Logistic Regression: max_iter={best_lr_iter}, score={best_lr_score:.4f}\")\n",
    "\n",
    "# 2. Test Random Forest\n",
    "print(\"\\n2. Testing Random Forest:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=100, n_jobs=-1)\n",
    "rf_scores = cross_val_score(rf, X_scaled, y_encoded, cv=cv, scoring='accuracy')\n",
    "rf_score_mean = rf_scores.mean()\n",
    "\n",
    "print(f\"Random Forest: CV Accuracy = {rf_score_mean:.4f} ± {rf_scores.std():.4f}\")\n",
    "\n",
    "# 3. Hyperparameter tuning for best model\n",
    "print(\"\\n3. Hyperparameter Tuning:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if rf_score_mean > best_lr_score:\n",
    "    print(\"Random Forest performed better - tuning RF parameters...\")\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=100, n_jobs=-1),\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_scaled, y_encoded)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(f\"Best RF params: {grid_search.best_params_}\")\n",
    "    print(f\"Best RF score: {best_score:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Logistic Regression performed better - tuning LR parameters...\")\n",
    "    \n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'max_iter': [best_lr_iter]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=100),\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_scaled, y_encoded)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(f\"Best LR params: {grid_search.best_params_}\")\n",
    "    print(f\"Best LR score: {best_score:.4f}\")\n",
    "\n",
    "# Final training and prediction\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Training final model with CV score: {best_score:.4f}\")\n",
    "best_model.fit(X_scaled, y_encoded)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "submission['class'] = [inverse_label_map[i] for i in y_pred_test]\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For logistic regression, show features with highest absolute coefficients\n",
    "    if len(best_model.coef_.shape) > 1:\n",
    "        # Multi-class: take mean of absolute coefficients across classes\n",
    "        coef_importance = np.mean(np.abs(best_model.coef_), axis=0)\n",
    "    else:\n",
    "        coef_importance = np.abs(best_model.coef_[0])\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': coef_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features (by coefficient magnitude):\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(f\"\\nSaved submission.csv with {len(submission)} predictions\")\n",
    "print(\"Model training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be37d835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T07:01:24.501232Z",
     "iopub.status.busy": "2025-06-16T07:01:24.500956Z",
     "iopub.status.idle": "2025-06-16T07:01:24.505730Z",
     "shell.execute_reply": "2025-06-16T07:01:24.504893Z"
    },
    "papermill": {
     "duration": 0.008837,
     "end_time": "2025-06-16T07:01:24.507052",
     "exception": false,
     "start_time": "2025-06-16T07:01:24.498215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'ID', '20150720_N', '20150602_N', '20150517_N', '20150501_N', '20150415_N', '20150330_N', '20150314_N', '20150226_N', '20150210_N', '20150125_N', '20150109_N', '20141117_N', '20141101_N', '20141016_N', '20140930_N', '20140813_N', '20140626_N', '20140610_N', '20140525_N', '20140509_N', '20140423_N', '20140407_N', '20140322_N', '20140218_N', '20140202_N', '20140117_N', '20140101_N', 'ndvi_mean', 'ndvi_std', 'ndvi_min', 'ndvi_max', 'ndvi_range', 'ndvi_median', 'ndvi_skew', 'ndvi_kurtosis', 'ndvi_q25', 'ndvi_q75', 'ndvi_iqr', 'ndvi_trend', 'early_mean', 'mid_mean', 'late_mean', 'early_trend', 'mid_trend', 'late_trend', 'early_late_diff', 'peak_position', 'valley_position', 'peak_valley_diff', 'ndvi_acceleration', 'ndvi_cv', 'above_median_ratio', 'trend_x_mean', 'range_x_skew', 'std_x_mean', 'peak_pos_x_max', 'log_std', 'log_range', 'log_iqr', 'ndvi_p10', 'ndvi_p90', 'extreme_range']\n"
     ]
    }
   ],
   "source": [
    "print(test_df.columns.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1OS0L4NP2srTN_ItrlZcCQL_aWvYGAg2A",
     "timestamp": 1749649895616
    }
   ]
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12585144,
     "sourceId": 104491,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 92.901012,
   "end_time": "2025-06-16T07:01:27.128872",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-16T06:59:54.227860",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
